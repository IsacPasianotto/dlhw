{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac72f528699a5b7c",
   "metadata": {},
   "source": [
    "# Deep Learning assignment #1\n",
    "\n",
    "***Student:*** [Isac Pasianotto](https://github.com/IsacPasianotto/dlhw)"
   ]
  },
  {
   "cell_type": "code",
   "id": "e1ff2359f4f3d087",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-04T16:18:10.747462Z"
    }
   },
   "source": [
    "import torch\n",
    "from typing import Iterable, List, Tuple\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14cc034649a1632a",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Some constants\n",
    "STD_DTYPE = torch.float32\n",
    "NDIGITS = 6\n",
    "N_HIDDEN = 2\n",
    "N_HEPOCHS = 1425"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b460cdffde328fe5",
   "metadata": {},
   "source": [
    "## 0. Dataset definition\n",
    "\n",
    "The considered dataset is the set of all possible 6-bit long binary sequences. Since the task the network is supposed to perform is to check if a given sequence is palindromic, the dataset is labeled accordingly."
   ]
  },
  {
   "cell_type": "code",
   "id": "a8b06e159606fdf",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def is_symmetric(iterable: Iterable) -> float:\n",
    "    \"\"\"\n",
    "    Check if an iterable is palindromic.\n",
    "    :param iterable: an iterable object to check (in our case, a torch tensor)\n",
    "    :return: 1 if the iterable is palindromic, 0 otherwise\n",
    "    \"\"\"\n",
    "    return 1. if all(a == b for a, b in zip(iterable, reversed(iterable))) else 0.\n",
    "\n",
    "# Ensure that the function works as expected\n",
    "assert is_symmetric( torch.tensor([0, 1, 0, 1, 1, 0]) ) == 0.\n",
    "assert is_symmetric( torch.tensor([0, 1, 0, 0, 1, 0]) ) == 1."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af937db0ff5741fe",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def build_dataset(nbits: int = NDIGITS, dtype: torch.dtype = STD_DTYPE , unbalanced: bool = True, seed: int = 42 ) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Build the dataset of all possible binary sequences of length nbits and their corresponding labels.\n",
    "    :param nbits: the length (integer) of the binary sequences. Default is 6.\n",
    "    :param dtype: the data type of the tensors. Default is torch.float32.\n",
    "    :param unbalanced: if True, the dataset will contain all the possible binary sequences of length nbits. If False, the palindromic sequences will be repeated more time to balance the dataset.\n",
    "    :param seed: the seed for the random number generator. Default is 42, which is the answer to the Ultimate Question of Life, the Universe, and Everything.\n",
    "    :return: Two tensors: the first one contains all the binary sequences, the second one contains the corresponding labels.\n",
    "    \"\"\"\n",
    "    bin_seq: List[Tuple[int]] = list(product([0, 1], repeat=nbits))  # generate all possible binary sequences of length nbits\n",
    "    sim_true: List[Tuple[int]] = [seq for seq in bin_seq if is_symmetric(seq)]\n",
    "    sim_false: List[Tuple[int]] = [seq for seq in bin_seq if seq not in sim_true]\n",
    "\n",
    "    sim_true: Tensor = torch.tensor(sim_true, dtype=dtype)\n",
    "    sim_false: Tensor = torch.tensor(sim_false, dtype=dtype)\n",
    "\n",
    "    if unbalanced:\n",
    "        x_data: Tensor = torch.cat([sim_true, sim_false], dim=0)\n",
    "        y_data: Tensor = torch.cat([torch.ones(sim_true.shape[0], dtype=dtype), torch.zeros(sim_false.shape[0], dtype=dtype)], dim=0)\n",
    "    else:\n",
    "        balance_ratio: int = sim_false.shape[0] // sim_true.shape[0]\n",
    "        x_data: Tensor = torch.cat([sim_false, torch.cat([sim_true]*balance_ratio, dim=0)], dim=0)\n",
    "        y_data: Tensor = torch.cat([torch.zeros(sim_false.shape[0], dtype=dtype), torch.ones(sim_true.shape[0]*balance_ratio, dtype=dtype)], dim=0)\n",
    "        \n",
    "    n_seq: int = x_data.shape[0]\n",
    "    rndidx: Tensor[int] = torch.randperm(n_seq, generator=torch.Generator().manual_seed(seed))\n",
    "    # Shuffle the dataset\n",
    "    x_data = x_data[rndidx]\n",
    "    y_data = y_data[rndidx]\n",
    "    \n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = build_dataset()\n",
    "x_data_balanced, y_data_balanced = build_dataset(unbalanced=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd030f21d8310c0f",
   "metadata": {},
   "source": [
    "## 1. Network definition\n",
    "\n",
    "The considered paper is quite cryptic regarding all the needed information to reproduce exactly the experiment. From that I have evinced that:\n",
    "- The input layer has 6 neurons, one for each bit of the input sequence.\n",
    "- There is just 1 hidden layer with 2 neurons.\n",
    "- The output layer has 1 neuron, which is the output of the network.\n",
    "\n",
    "Hence, we can re-draw the *Fig. 1* of the paper with a more conventional representation as:  \n",
    "\n",
    "<center><img src=\"./images/img-00.png\" style=\"width: 35%;\"/></center>\n",
    "\n",
    "Moreover, there is specified that: \n",
    "- The initial weights are randomly drawn from a uniform distribution in the range [-0.3, 0.3].\n",
    "- The gradient descent algorithm was not stochastic, all the dataset was used to compute the gradient.\n",
    "- The loss function used (called \"Error\" in the paper) is $\\frac{1}{2}\\sum_{c}\\sum_{j}{\\left( y_{i,c} - d_{i, c} \\right)^2}$\n",
    "- The train lasted for 1,425 epochs.\n",
    "- The learning rate was $\\varepsilon = 0.1$.\n",
    "\n",
    "\n",
    "What is not specified is: \n",
    "\n",
    "- The activation function used in the hidden layer. The only mentioned non-linear function in the paper is $y_i = \\frac{1}{1+e^{-x_i}}$ which is the sigmoid function and is already implemented in [PyTorch](https://pytorch.org/docs/stable/generated/https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.htmll).\n",
    "- The initialization of the bias terms. I will assume that they are initialized to 0."
   ]
  },
  {
   "cell_type": "code",
   "id": "fe33e035bb1a7d8a",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "class paper_net(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The network as described in the paper. Class which extends torch.nn.Module.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int = NDIGITS, hidden_size: int = 2, func: torch.nn.Module = torch.nn.Sigmoid(), dtype: torch.dtype = STD_DTYPE):\n",
    "        \"\"\"\n",
    "        Initialize the network object.\n",
    "        :param input_size: the size of the input layer. Should be equal to te number of bits of the input sequence. Default is 6.\n",
    "        :param hidden_size: the size of the hidden layer. Default is 2.\n",
    "        :param func: the activation function to use in the hidden layer. Default is torch.nn.Sigmoid.\n",
    "        :param dtype: the data type of the tensors. Default is torch.float32.\n",
    "        \"\"\"\n",
    "        super(paper_net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_size = 1\n",
    "        self.act_fun = func\n",
    "        self.dtype = dtype\n",
    "        self.w_min = -0.3\n",
    "        self.w_max = 0.3\n",
    "        self.b = 0.\n",
    "        \n",
    "        self.hidden = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.output = torch.nn.Linear(self.hidden_size, self.out_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Weights initialization\n",
    "            torch.nn.init.uniform_(self.hidden.weight,self.w_min, self.w_max)\n",
    "            torch.nn.init.uniform_(self.output.weight,self.w_min, self.w_max)\n",
    "            # Bias initialization\n",
    "            torch.nn.init.constant_(self.hidden.bias, self.b)\n",
    "            torch.nn.init.constant_(self.output.bias, self.b)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        :param x: the input tensor.\n",
    "        :return: the output tensor.\n",
    "        \"\"\"\n",
    "        x = self.act_fun(self.hidden(x))\n",
    "        x = self.act_fun(self.output(x))\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b7a174e9674cd2e",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def loss_fn(y_pred: Tensor, y_true: Tensor, dtype: torch.dtype = STD_DTYPE) -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute the loss function.\n",
    "    :param y_pred: the predicted values.\n",
    "    :param y_true: the true values.\n",
    "    :param dtype: the data type of the tensors. Default is torch.float32.\n",
    "    :return: the loss value.\n",
    "    \"\"\"\n",
    "    return .5 * torch.sum(torch.pow(y_pred - y_true, 2), dtype=dtype)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b72a9201467d666d",
   "metadata": {},
   "source": [
    "#### 1.1 Training the network, storing the weights and the losses\n",
    "\n",
    "Since is required to inspect the weights of the network during the training, I will define a custom train function to store at each iteration all the data I need."
   ]
  },
  {
   "cell_type": "code",
   "id": "3a98031259e3a662",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Set the batch size to the number of samples in the dataset to use the whole dataset at each iteration.\n",
    "# This guarantees that GD is performed instead of SGD.\n",
    "data: DataLoader = DataLoader(TensorDataset(x_data, y_data), batch_size=x_data.shape[0])\n",
    "data_balanced: DataLoader = DataLoader(TensorDataset(x_data_balanced, y_data_balanced), batch_size=x_data_balanced.shape[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eb306870a79f3eb",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def train_net(net: torch.nn.Module,\n",
    "              data: DataLoader,\n",
    "              hidden_layer_weights: Tensor,\n",
    "              output_layer_weights: Tensor,\n",
    "              accuracy: Tensor,\n",
    "              losses: Tensor,\n",
    "              n_epochs: int = N_HEPOCHS,\n",
    "              lr: float = 0.1,\n",
    "              optimizer: torch.optim.Optimizer = torch.optim.SGD,\n",
    "              dtype: torch.dtype = STD_DTYPE,\n",
    "              ): # -> None:\n",
    "    \"\"\"\n",
    "    Train the network and stores the weights of the network, the losses and the accuracy in the given tensors.\n",
    "    :param net: the network to train.\n",
    "    :param data: the DataLoader object containing the dataset.\n",
    "    :param hidden_layer_weights: the tensor to store the hidden layer weights.\n",
    "    :param output_layer_weights: the tensor to store the output layer weights.\n",
    "    :param accuracy: the tensor to store the accuracy.\n",
    "    :param losses: the tensor to store the losses.\n",
    "    :param n_epochs: the number of epochs. Default is 1425.\n",
    "    :param lr: the learning rate. Default is 0.1.\n",
    "    :param optimizer: the optimizer to use. Default is torch.optim.SGD.\n",
    "    :param dtype: the data type of the tensors. Default is torch.float32.\n",
    "    :return: Four tensors: the hidden layer weights, the output layer weights, the accuracy and the losses.\n",
    "    \"\"\"\n",
    "    optimizer = optimizer(net.parameters(), lr=lr)\n",
    "    net.train()\n",
    "  \n",
    "    for epoch in range(n_epochs):\n",
    "        correct_predictions = 0        \n",
    "        for x, y in data:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = net(x)\n",
    "            l = loss_fn(y_pred, y, dtype=dtype)\n",
    "            losses[epoch] = l.item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred: Tensor = torch.round(y_pred)\n",
    "                acc: Tensor = torch.sum(pred == y, dtype=dtype) / y.shape[0]\n",
    "                accuracy[epoch] = acc\n",
    "\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            hidden_layer_weights[epoch] = net.hidden.weight.data\n",
    "            output_layer_weights[epoch] = net.output.weight.data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "946c1c5405733568",
   "metadata": {},
   "source": [
    "#### 1.2 Plotting the results\n",
    "\n",
    "Before continuing with the experiment, I will define a function to plot the results of the training. This will be handy to compare the results of different variations of the network."
   ]
  },
  {
   "cell_type": "code",
   "id": "65f55703b9b77fe6",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def plot_all(h_lay: Tensor, o_lay: Tensor, acc: Tensor, loss: Tensor, title: str) -> None:\n",
    "    \"\"\"\n",
    "    Make a 2x2 plot with the hidden layer weights, the output layer weights, the accuracy and the loss.\n",
    "    :param h_lay: Tensor containing the hidden layer weights at each epoch.\n",
    "    :param o_lay: Tensor containing the output layer weights at each epoch.\n",
    "    :param acc: Tensor containing the accuracy at each epoch.\n",
    "    :param loss: Tensor containing the loss at each epoch.\n",
    "    :param title: String containing the title of the image generated\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    # constants\n",
    "    epochs = range(len(acc))\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan']\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    h_lay_00 = h_lay[:, 0, :] # first neuron of the hidden layer\n",
    "    h_lay_01 = h_lay[:, 1, :] # second neuron of the hidden layer\n",
    "    \n",
    "    \n",
    "    ## Top-left: Accuracy & Loss\n",
    "    axs[0, 0].plot(epochs, acc, color=colors[0])\n",
    "    axs[0, 0].set_xlabel('Epochs', fontsize=14, fontweight='bold')\n",
    "    axs[0, 0].set_ylabel('Accuracy in %', color=colors[0], fontsize=14, fontweight='bold', labelpad=10)\n",
    "    axs[0, 0].tick_params('y', colors=colors[0], labelsize=10)\n",
    "    for label in axs[0, 0].get_yticklabels(): label.set_fontweight('bold')\n",
    "    axs[0, 0].set_title('Accuracy and Loss over Epochs', fontsize=16, fontweight='bold')\n",
    "    twinx = axs[0, 0].twinx()\n",
    "    twinx.plot(epochs, loss, color=colors[1])\n",
    "    twinx.set_ylabel('Loss', color=colors[1], fontsize=15, fontweight='bold', labelpad=10)\n",
    "    twinx.tick_params('y', colors=colors[1], labelsize=10)\n",
    "    for label in twinx.get_yticklabels(): label.set_fontweight('bold')\n",
    "\n",
    "    ## Top-right: Hidden layer weights\n",
    "    \n",
    "    for i in range(NDIGITS):\n",
    "        axs[0, 1].plot(epochs, h_lay_00[:, i], color=colors[i], label=f'Neuron {i}')\n",
    "    axs[0, 1].set_xlabel('Epochs', fontsize=12, fontweight='bold')\n",
    "    axs[0, 1].set_ylabel('Weights', fontsize=12, fontweight='bold', labelpad=10)\n",
    "    axs[0, 1].set_title('Hidden layer weights over Epochs: 1', fontsize=16, fontweight='bold')\n",
    "    axs[0, 1].legend(loc='upper right', fontsize=9)\n",
    "    \n",
    "    ## Bottom-left: Hidden layer weights\n",
    "    \n",
    "    for i in range(NDIGITS):\n",
    "        axs[1, 0].plot(epochs, h_lay_01[:, i], color=colors[i], label=f'Neuron {i}')\n",
    "    axs[1, 0].set_xlabel('Epochs', fontsize=12, fontweight='bold')\n",
    "    axs[1, 0].set_ylabel('Weights', fontsize=12, fontweight='bold', labelpad=10)\n",
    "    axs[1, 0].set_title('Hidden layer weights over Epochs: 2', fontsize=16, fontweight='bold')\n",
    "    axs[1, 0].legend(loc='upper right', fontsize=9)\n",
    "    \n",
    "    ## Bottom-right: Output layer weights\n",
    "    for i in range(N_HIDDEN):\n",
    "        axs[1, 1].plot(epochs, o_lay[:, 0, i], color=colors[i], label=f'Neuron {i}')\n",
    "    axs[1, 1].set_xlabel('Epochs', fontsize=12, fontweight='bold')\n",
    "    axs[1, 1].set_ylabel('Weights', fontsize=12, fontweight='bold', labelpad=10)\n",
    "    axs[1, 1].set_title('Output layer weights over Epochs', fontsize=16, fontweight='bold')\n",
    "    axs[1, 1].legend(loc='upper right', fontsize=9)\n",
    "    \n",
    "    fig.suptitle(title, fontsize=20, fontweight='bold')\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.3, hspace=0.3)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "10241238ee9dd948",
   "metadata": {},
   "source": [
    "## 2 Perform the experiment\n",
    "\n",
    "#### 2.1 Original formulation\n",
    "\n",
    "We will use the `unbalanced dataset` to train the network:"
   ]
  },
  {
   "cell_type": "code",
   "id": "b53db299ea3f202d",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "model_00: paper_net = paper_net()\n",
    "h_lay_00: Tensor = torch.zeros(N_HEPOCHS, N_HIDDEN, NDIGITS, dtype=STD_DTYPE)\n",
    "o_lay_00: Tensor = torch.zeros(N_HEPOCHS, 1, N_HIDDEN, dtype=STD_DTYPE)\n",
    "acc_00: Tensor = torch.zeros(N_HEPOCHS, dtype=STD_DTYPE)\n",
    "loss_00: Tensor = torch.zeros(N_HEPOCHS, dtype=STD_DTYPE)\n",
    "\n",
    "train_net(model_00, data, h_lay_00, o_lay_00, acc_00, loss_00)\n",
    "plot_all(h_lay_00, o_lay_00, acc_00, loss_00, \"Original formulation\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "113f5813b30463f7",
   "metadata": {},
   "source": [
    "#### 2.2 Balanced dataset\n",
    "\n",
    "Use the same network but with the `balanced dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "id": "8bd717acd9fb4f53",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "model_01: paper_net = paper_net()\n",
    "h_lay_01: Tensor = torch.zeros(N_HEPOCHS, N_HIDDEN, NDIGITS, dtype=STD_DTYPE)\n",
    "o_lay_01: Tensor = torch.zeros(N_HEPOCHS, 1, N_HIDDEN, dtype=STD_DTYPE)\n",
    "acc_01: Tensor = torch.zeros(N_HEPOCHS, dtype=STD_DTYPE)\n",
    "loss_01: Tensor = torch.zeros(N_HEPOCHS, dtype=STD_DTYPE)\n",
    "\n",
    "train_net(model_01, data_balanced, h_lay_01, o_lay_01, acc_01, loss_01)\n",
    "plot_all(h_lay_01, o_lay_01, acc_01, loss_01, \"Balanced dataset\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "84eb5bfa870e06c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
